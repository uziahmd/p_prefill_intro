{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efabcdc-c865-49b2-8bd3-2b8b35ec37d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count truncated initial_response per file using TOKEN-COUNT-ONLY detection\n",
    "# (No \"unfinished ending\" checks; max_new_tokens in JSONL is ignored.)\n",
    "#\n",
    "# Truncated criterion:\n",
    "#   tokenize(initial_response) minus tokenize(prefill_text if it is a true prefix)\n",
    "#   => gen_token_count\n",
    "#   truncated if gen_token_count is within MARGIN of any value in LIMIT_GRID.\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Iterable, Tuple\n",
    "from functools import lru_cache\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG\n",
    "# -----------------------\n",
    "DATA_DIR = Path(\"rq1_runs\")\n",
    "\n",
    "FILES_TO_PROCESS = [\n",
    "    \"gen_google__gemma-3-4b-it_harmbench.jsonl\",\n",
    "    \"gen_google__gemma-3-4b-it.jsonl\",\n",
    "    \"gen_google__gemma-3-12b-it_harmbench.jsonl\",\n",
    "    \"gen_google__gemma-3-12b-it.jsonl\",\n",
    "    \"gen_google__gemma-3-27b-it_harmbench.jsonl\",\n",
    "    \"gen_google__gemma-3-27b-it.jsonl\",\n",
    "    \"gen_meta-llama__Llama-3.1-8B-Instruct_harmbench.jsonl\",\n",
    "    \"gen_meta-llama__Llama-3.1-8B-Instruct.jsonl\",\n",
    "    \"gen_meta-llama__llama-3.3-70b-instruct_harmbench.jsonl\",\n",
    "    \"gen_meta-llama__llama-3.3-70b-instruct.jsonl\",\n",
    "    \"gen_Qwen__Qwen3-8B_harmbench.jsonl\",\n",
    "    \"gen_Qwen__Qwen3-8B.jsonl\",\n",
    "    \"gen_Qwen__Qwen3-14B_harmbench.jsonl\",\n",
    "    \"gen_Qwen__Qwen3-14B.jsonl\",\n",
    "    \"gen_qwen__qwen3-32b_harmbench.jsonl\",\n",
    "    \"gen_qwen__qwen3-32b.jsonl\",\n",
    "]\n",
    "\n",
    "# The real caps you believe were enforced by the generator\n",
    "LIMIT_GRID = [512, 1024, 1280, 1536, 1792]\n",
    "\n",
    "# Error margin (tokens) around each cap\n",
    "MARGIN = 4  # tune: 1–2 = stricter, 4–8 = more recall\n",
    "\n",
    "FIELD = \"initial_response\"\n",
    "\n",
    "# Optional: if your JSONL model_name isn't a valid HF tokenizer id, map it here.\n",
    "# Example (fill only if needed):\n",
    "MODEL_NAME_OVERRIDES = {\n",
    "    # \"google/gemma-3-12b-it\": \"google/gemma-3-12b-it\",\n",
    "    # \"gen_meta-llama__Llama-3.1-8B-Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# IO\n",
    "# -----------------------\n",
    "def iter_jsonl(path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line_no, line in enumerate(f, 1):\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip malformed lines; keep going\n",
    "                continue\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Tokenizer cache\n",
    "# -----------------------\n",
    "@lru_cache(maxsize=64)\n",
    "def get_tokenizer(model_name: str):\n",
    "    tok_name = MODEL_NAME_OVERRIDES.get(model_name, model_name)\n",
    "    return AutoTokenizer.from_pretrained(tok_name, trust_remote_code=True)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Token counting (prefill-safe)\n",
    "# -----------------------\n",
    "def generated_token_count(rec: Dict[str, Any], tokenizer, field: str = FIELD) -> int:\n",
    "    \"\"\"\n",
    "    Compute the token count generated by the model for `field`.\n",
    "    If prefill_text is literally included as a prefix, subtract it (safely).\n",
    "    \"\"\"\n",
    "    full_text = rec.get(field) or \"\"\n",
    "    if not full_text:\n",
    "        return 0\n",
    "\n",
    "    prefill = rec.get(\"prefill_text\") or \"\"\n",
    "    full_ids = tokenizer.encode(full_text, add_special_tokens=False)\n",
    "\n",
    "    if prefill and full_text.startswith(prefill):\n",
    "        pre_ids = tokenizer.encode(prefill, add_special_tokens=False)\n",
    "\n",
    "        # safest: subtract only if token-prefix matches\n",
    "        if full_ids[: len(pre_ids)] == pre_ids:\n",
    "            return max(0, len(full_ids) - len(pre_ids))\n",
    "\n",
    "        # fallback: tokenize the string suffix (stable even if token-prefix mismatch)\n",
    "        suffix = full_text[len(prefill) :]\n",
    "        return len(tokenizer.encode(suffix, add_special_tokens=False))\n",
    "\n",
    "    # no usable prefill subtraction\n",
    "    return len(full_ids)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Token-only truncation decision\n",
    "# -----------------------\n",
    "def is_truncated_token_only(rec: Dict[str, Any], tokenizer) -> bool:\n",
    "    gen_ct = generated_token_count(rec, tokenizer, field=FIELD)\n",
    "    return any(abs(gen_ct - L) <= MARGIN for L in LIMIT_GRID)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Per-file counting\n",
    "# -----------------------\n",
    "def count_truncations_for_file(path: Path) -> Tuple[int, int]:\n",
    "    total = 0\n",
    "    trunc = 0\n",
    "\n",
    "    # We'll fetch tokenizer per record (cached), in case a file mixes models.\n",
    "    for rec in iter_jsonl(path):\n",
    "        total += 1\n",
    "        model_name = rec.get(\"model_name\")\n",
    "        if not model_name:\n",
    "            continue  # skip if malformed\n",
    "        tok = get_tokenizer(model_name)\n",
    "        if is_truncated_token_only(rec, tok):\n",
    "            trunc += 1\n",
    "\n",
    "    return total, trunc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca667cea-40e2-4382-a40d-d7800bb3f4b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Token-only truncation summary (initial_response):\n",
      "file                                                                       total     trunc        %\n",
      "----------------------------------------------------------------------------------------------------\n",
      "gen_google__gemma-3-4b-it.jsonl                                             2920      2915   99.83%\n",
      "gen_google__gemma-3-4b-it_harmbench.jsonl                                   2000      1853   92.65%\n",
      "gen_google__gemma-3-12b-it.jsonl                                            2920      2647   90.65%\n",
      "gen_google__gemma-3-27b-it.jsonl                                            2920      2469   84.55%\n",
      "gen_google__gemma-3-12b-it_harmbench.jsonl                                  2000      1676   83.80%\n",
      "gen_google__gemma-3-27b-it_harmbench.jsonl                                  2000      1342   67.10%\n",
      "gen_meta-llama__Llama-3.1-8B-Instruct.jsonl                                 2920      1493   51.13%\n",
      "gen_qwen__qwen3-32b.jsonl                                                   2920      1364   46.71%\n",
      "gen_Qwen__Qwen3-8B.jsonl                                                    2920      1212   41.51%\n",
      "gen_meta-llama__Llama-3.1-8B-Instruct_harmbench.jsonl                       2000       797   39.85%\n",
      "gen_Qwen__Qwen3-VL-8B-Instruct_harmbench.jsonl                              2000       783   39.15%\n",
      "gen_Qwen__Qwen3-8B_harmbench.jsonl                                          2000       747   37.35%\n",
      "gen_Qwen__Qwen3-VL-8B-Instruct.jsonl                                        2920      1090   37.33%\n",
      "gen_Qwen__Qwen3-14B.jsonl                                                   2920      1052   36.03%\n",
      "gen_Qwen__Qwen3-14B_harmbench.jsonl                                         2000       575   28.75%\n",
      "gen_qwen__qwen3-32b_harmbench.jsonl                                         2000       568   28.40%\n",
      "gen_meta-llama__llama-3.3-70b-instruct_harmbench.jsonl                      2000       551   27.55%\n",
      "gen_meta-llama__llama-3.3-70b-instruct.jsonl                                2920       751   25.72%\n",
      "\n",
      "Settings used:\n",
      "  LIMIT_GRID = [512, 1024, 1280, 1536, 1792]\n",
      "  MARGIN     = 4\n",
      "  FIELD      = initial_response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rows = []\n",
    "missing = []\n",
    "\n",
    "for fname in FILES_TO_PROCESS:\n",
    "    path = DATA_DIR / fname\n",
    "    if not path.exists():\n",
    "        missing.append(str(path))\n",
    "        continue\n",
    "\n",
    "    total, trunc = count_truncations_for_file(path)\n",
    "    pct = (100.0 * trunc / total) if total else 0.0\n",
    "    rows.append((fname, total, trunc, pct))\n",
    "\n",
    "if missing:\n",
    "    print(\"\\nMissing files:\")\n",
    "    for m in missing:\n",
    "        print(\"  -\", m)\n",
    "\n",
    "if not rows:\n",
    "    print(\"\\nNo files processed.\")\n",
    "\n",
    "# Sort by truncation rate descending\n",
    "rows.sort(key=lambda x: x[3], reverse=True)\n",
    "\n",
    "print(\"\\nToken-only truncation summary (initial_response):\")\n",
    "print(f\"{'file':70}  {'total':>8}  {'trunc':>8}  {'%':>7}\")\n",
    "print(\"-\" * 100)\n",
    "for fname, total, trunc, pct in rows:\n",
    "    print(f\"{fname:70}  {total:8d}  {trunc:8d}  {pct:6.2f}%\")\n",
    "\n",
    "print(\"\\nSettings used:\")\n",
    "print(\"  LIMIT_GRID =\", LIMIT_GRID)\n",
    "print(\"  MARGIN     =\", MARGIN)\n",
    "print(\"  FIELD      =\", FIELD)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a538a4-a5e1-405b-91f3-e272cd49df6e",
   "metadata": {},
   "source": [
    "# Fix code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52970750-1f22-43a8-ad8e-ef45728cb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 1: Setup & Config\n",
    "# =========================\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import inspect\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple, Iterable\n",
    "from functools import lru_cache\n",
    "from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED\n",
    "\n",
    "import requests\n",
    "import httpx\n",
    "from tqdm.auto import tqdm\n",
    "from openai import OpenAI\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "RUN_DIR = Path(\"rq1_runs\")\n",
    "\n",
    "# Two files pertaining to the same model to run back-to-back\n",
    "FILES_TO_PROCESS = [\n",
    "    RUN_DIR / \"gen_Qwen__Qwen3-14B_harmbench.jsonl\",\n",
    "    RUN_DIR / \"gen_Qwen__Qwen3-14B.jsonl\"\n",
    "]\n",
    "\n",
    "# SGLang OpenAI-compatible endpoint\n",
    "SGLANG_BASE_URL = \"http://127.0.0.1:30000/v1\"\n",
    "SGLANG_API_KEY = \"EMPTY\"\n",
    "\n",
    "# Global continuation limits\n",
    "MAX_TOTAL_TOKENS = 2048\n",
    "FOLLOWUP_MAX_TOKENS = 512\n",
    "\n",
    "# Truncation detection limits (from your detection script)\n",
    "LIMIT_GRID = [512, 1024, 1280, 1536, 1792]\n",
    "MARGIN = 4\n",
    "\n",
    "# Keep-alive / connection pooling for faster repeated requests\n",
    "_http_client = httpx.Client(\n",
    "    limits=httpx.Limits(max_connections=64, max_keepalive_connections=16),\n",
    "    timeout=httpx.Timeout(60.0),\n",
    ")\n",
    "sg = OpenAI(base_url=SGLANG_BASE_URL, api_key=SGLANG_API_KEY, http_client=_http_client)\n",
    "\n",
    "# Parallelism settings\n",
    "MAX_WORKERS = 8\n",
    "MAX_IN_FLIGHT = MAX_WORKERS * 4\n",
    "DO_MERGE = True\n",
    "\n",
    "def is_blank(x: Optional[str]) -> bool:\n",
    "    return x is None or (isinstance(x, str) and x.strip() == \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93e229be-fb1d-43a3-bd44-86a19f735255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 2: Detection Logic\n",
    "# =========================\n",
    "\n",
    "@lru_cache(maxsize=8)\n",
    "def get_tokenizer(model_name: str):\n",
    "    return AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "def generated_token_count(rec: Dict[str, Any], tokenizer, field: str = \"initial_response\") -> int:\n",
    "    \"\"\"\n",
    "    Compute the token count generated by the model.\n",
    "    Subtracts prefill safely if it exists.\n",
    "    \"\"\"\n",
    "    full_text = rec.get(field) or \"\"\n",
    "    if not full_text:\n",
    "        return 0\n",
    "\n",
    "    prefill = rec.get(\"prefill_text\") or \"\"\n",
    "    full_ids = tokenizer.encode(full_text, add_special_tokens=False)\n",
    "\n",
    "    if prefill and full_text.startswith(prefill):\n",
    "        pre_ids = tokenizer.encode(prefill, add_special_tokens=False)\n",
    "        # safest: subtract only if token-prefix matches\n",
    "        if full_ids[: len(pre_ids)] == pre_ids:\n",
    "            return max(0, len(full_ids) - len(pre_ids))\n",
    "\n",
    "        # fallback: tokenize the string suffix\n",
    "        suffix = full_text[len(prefill) :]\n",
    "        return len(tokenizer.encode(suffix, add_special_tokens=False))\n",
    "\n",
    "    return len(full_ids)\n",
    "\n",
    "def is_truncated_token_only(rec: Dict[str, Any], tokenizer) -> bool:\n",
    "    \"\"\"Returns True if the generated token count is within MARGIN of any known limit.\"\"\"\n",
    "    if rec.get(\"condition\") == \"error\":\n",
    "        return False\n",
    "    gen_ct = generated_token_count(rec, tokenizer)\n",
    "    return any(abs(gen_ct - L) <= MARGIN for L in LIMIT_GRID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "650030fb-59fe-49d1-8341-f6f53547546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 3: SGLang API & Prompting\n",
    "# =========================\n",
    "_SERVER_MODEL_ID = None\n",
    "\n",
    "def get_server_model_id() -> str:\n",
    "    global _SERVER_MODEL_ID\n",
    "    if _SERVER_MODEL_ID is not None:\n",
    "        return _SERVER_MODEL_ID\n",
    "    models = requests.get(SGLANG_BASE_URL + \"/models\", timeout=10).json()\n",
    "    _SERVER_MODEL_ID = models[\"data\"][0][\"id\"]\n",
    "    return _SERVER_MODEL_ID\n",
    "\n",
    "def apply_chat_template_no_think_if_supported(tokenizer, messages):\n",
    "    try:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "            enable_thinking=False,\n",
    "        )\n",
    "    except TypeError:\n",
    "        return tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            add_generation_prompt=True,\n",
    "            tokenize=False,\n",
    "        )\n",
    "\n",
    "def build_base_prompt(tokenizer, user_text: str) -> str:\n",
    "    messages = [{\"role\": \"user\", \"content\": user_text}]\n",
    "    return apply_chat_template_no_think_if_supported(tokenizer, messages)\n",
    "\n",
    "def build_followup_prompt(tokenizer, user_text: str, assistant_text: str, followup_text: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": user_text},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_text},\n",
    "        {\"role\": \"user\", \"content\": followup_text},\n",
    "    ]\n",
    "    return apply_chat_template_no_think_if_supported(tokenizer, messages)\n",
    "\n",
    "def sglang_complete(prompt: str, max_new_tokens: int, temperature: float = 0.0, top_p: float = 1.0) -> str:\n",
    "    last_err = None\n",
    "    for attempt in range(6):\n",
    "        try:\n",
    "            resp = sg.completions.create(\n",
    "                model=get_server_model_id(),\n",
    "                prompt=prompt,\n",
    "                max_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "            )\n",
    "            return resp.choices[0].text\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            time.sleep(1.0 * (2 ** attempt) + random.random() * 0.25)\n",
    "    raise RuntimeError(f\"SGLang completion failed. last_err={repr(last_err)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb3c0fc2-d99d-4354-a4f5-55857d53e846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CELL 4: The Core Worker\n",
    "# =========================\n",
    "\n",
    "def process_record(rec: Dict[str, Any], tokenizer) -> Tuple[Dict[str, Any], bool]:\n",
    "    \"\"\"\n",
    "    Checks if truncated. If so, calculates remaining tokens to reach 2048,\n",
    "    continues the initial_response, and regenerates the followup_response.\n",
    "    \"\"\"\n",
    "    if not is_truncated_token_only(rec, tokenizer):\n",
    "        return rec, False\n",
    "\n",
    "    user_text = rec.get(\"prompt_text\") or \"\"\n",
    "    followup_text = rec.get(\"followup_text\") or \"\"\n",
    "    initial_text = rec.get(\"initial_response\") or \"\"\n",
    "    \n",
    "    if is_blank(user_text) or is_blank(followup_text) or is_blank(initial_text):\n",
    "        return rec, False\n",
    "\n",
    "    # 1. Calculate remaining budget\n",
    "    gen_ct = generated_token_count(rec, tokenizer)\n",
    "    remaining_tokens = MAX_TOTAL_TOKENS - gen_ct\n",
    "    \n",
    "    # If it's already over or exactly at the limit, we skip it\n",
    "    if remaining_tokens <= 0:\n",
    "        return rec, False\n",
    "\n",
    "    base_kwargs = rec.get(\"gen_kwargs\", {})\n",
    "    temperature = float(base_kwargs.get(\"temperature\", 0.0))\n",
    "    top_p = float(base_kwargs.get(\"top_p\", 1.0))\n",
    "\n",
    "    # 2. Continue the Initial Response\n",
    "    base_prompt = build_base_prompt(tokenizer, user_text)\n",
    "    \n",
    "    # By appending the raw initial_response to the base prompt, SGLang \n",
    "    # will literally start generating the very next token.\n",
    "    continuation_prompt = base_prompt + initial_text\n",
    "    \n",
    "    continuation_suffix = sglang_complete(\n",
    "        prompt=continuation_prompt, \n",
    "        max_new_tokens=remaining_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "    new_initial = initial_text + continuation_suffix\n",
    "\n",
    "    # 3. Regenerate the Followup Response\n",
    "    followup_prompt = build_followup_prompt(tokenizer, user_text, new_initial, followup_text)\n",
    "    new_followup = sglang_complete(\n",
    "        prompt=followup_prompt, \n",
    "        max_new_tokens=FOLLOWUP_MAX_TOKENS,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p\n",
    "    )\n",
    "\n",
    "    # 4. Patch record in-place\n",
    "    rec[\"initial_response\"] = new_initial\n",
    "    rec[\"followup_response\"] = new_followup\n",
    "    \n",
    "    return rec, True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3e3d64-ec68-48b9-a290-5b577a5e663a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing File: gen_Qwen__Qwen3-14B_harmbench.jsonl\n",
      "==================================================\n",
      "Total records: 2000\n",
      "Truncated candidates to continue: 575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b13072b65fd84d49b7e39907c5b959ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2000 [00:00<?, ?rec/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing gen_Qwen__Qwen3-14B_harmbench.jsonl.\n",
      "Patched records written: 575 | Errors: 0\n",
      "Merged output saved to: rq1_runs/gen_Qwen__Qwen3-14B_harmbench_continued_merged.jsonl\n",
      "Total replaced records in merge: 575\n",
      "\n",
      "==================================================\n",
      "Processing File: gen_Qwen__Qwen3-14B.jsonl\n",
      "==================================================\n",
      "Total records: 2920\n",
      "Truncated candidates to continue: 1052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7506507e8abc4384945532af3fa5645c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/2920 [00:00<?, ?rec/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing gen_Qwen__Qwen3-14B.jsonl.\n",
      "Patched records written: 1052 | Errors: 0\n",
      "Merged output saved to: rq1_runs/gen_Qwen__Qwen3-14B_continued_merged.jsonl\n",
      "Total replaced records in merge: 1052\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# CELL 5: Execution & Merging\n",
    "# =========================\n",
    "def iter_jsonl(path: Path) -> Iterable[Dict[str, Any]]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                yield json.loads(line)\n",
    "\n",
    "def record_key(r: Dict[str, Any]) -> Tuple[Any, ...]:\n",
    "    return (r.get(\"model_name\"), r.get(\"prompt_id\"), r.get(\"condition\"), r.get(\"prefill_id\"), r.get(\"followup_id\"))\n",
    "\n",
    "def peek_first_record(path: Path) -> Dict[str, Any]:\n",
    "    for r in iter_jsonl(path):\n",
    "        return r\n",
    "    raise ValueError(f\"Empty JSONL: {path}\")\n",
    "\n",
    "# Run back-to-back\n",
    "for in_path in FILES_TO_PROCESS:\n",
    "    if not in_path.exists():\n",
    "        print(f\"Skipping {in_path} (File not found)\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{'='*50}\\nProcessing File: {in_path.name}\\n{'='*50}\")\n",
    "\n",
    "    patch_path = Path(str(in_path).replace(\".jsonl\", \"_continued_patch.jsonl\"))\n",
    "    merged_path = Path(str(in_path).replace(\".jsonl\", \"_continued_merged.jsonl\"))\n",
    "\n",
    "    # Validate model matching\n",
    "    file_model = peek_first_record(in_path).get(\"model_name\")\n",
    "    server_model = get_server_model_id()\n",
    "    \n",
    "    if file_model != server_model:\n",
    "        print(f\"ERROR: Model mismatch for {in_path.name}.\")\n",
    "        print(f\"  File model:   {file_model}\\n  Server model: {server_model}\")\n",
    "        print(\"Skipping to next file...\")\n",
    "        continue\n",
    "\n",
    "    tokenizer = get_tokenizer(file_model)\n",
    "\n",
    "    # Pre-count total & truncated\n",
    "    total_records = 0\n",
    "    trunc_candidates = 0\n",
    "    for rec in iter_jsonl(in_path):\n",
    "        total_records += 1\n",
    "        if is_truncated_token_only(rec, tokenizer):\n",
    "            trunc_candidates += 1\n",
    "\n",
    "    print(f\"Total records: {total_records}\")\n",
    "    print(f\"Truncated candidates to continue: {trunc_candidates}\")\n",
    "\n",
    "    # Process and write\n",
    "    if patch_path.exists():\n",
    "        patch_path.unlink()\n",
    "\n",
    "    patched, errors = 0, 0\n",
    "    cand_idx, next_flush = 0, 0\n",
    "    futures_by_idx, results_by_idx = {}, {}\n",
    "\n",
    "    pbar = tqdm(iter_jsonl(in_path), total=total_records, desc=\"Processing\", unit=\"rec\")\n",
    "    \n",
    "    with patch_path.open(\"a\", encoding=\"utf-8\") as fpatch:\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as ex:\n",
    "            for rec in pbar:\n",
    "                # Only submit truncated records\n",
    "                if is_truncated_token_only(rec, tokenizer):\n",
    "                    futures_by_idx[cand_idx] = ex.submit(process_record, rec, tokenizer)\n",
    "                    cand_idx += 1\n",
    "\n",
    "                # Flush logic\n",
    "                while len(futures_by_idx) >= MAX_IN_FLIGHT:\n",
    "                    done_set, _ = wait(list(futures_by_idx.values()), return_when=FIRST_COMPLETED)\n",
    "                    for i in [idx for idx, fut in futures_by_idx.items() if fut in done_set]:\n",
    "                        try:\n",
    "                            results_by_idx[i] = futures_by_idx.pop(i).result()\n",
    "                        except Exception as e:\n",
    "                            results_by_idx[i] = e\n",
    "\n",
    "                    while next_flush in results_by_idx:\n",
    "                        out = results_by_idx.pop(next_flush)\n",
    "                        if isinstance(out, Exception):\n",
    "                            errors += 1\n",
    "                        else:\n",
    "                            rec2, did_patch = out\n",
    "                            if did_patch:\n",
    "                                patched += 1\n",
    "                                fpatch.write(json.dumps(rec2, ensure_ascii=False) + \"\\n\")\n",
    "                        next_flush += 1\n",
    "\n",
    "                pbar.set_postfix(cands=trunc_candidates, patched=patched, errs=errors)\n",
    "\n",
    "            # Drain remaining\n",
    "            while futures_by_idx:\n",
    "                done_set, _ = wait(list(futures_by_idx.values()), return_when=FIRST_COMPLETED)\n",
    "                for i in [idx for idx, fut in futures_by_idx.items() if fut in done_set]:\n",
    "                    try:\n",
    "                        results_by_idx[i] = futures_by_idx.pop(i).result()\n",
    "                    except Exception as e:\n",
    "                        results_by_idx[i] = e\n",
    "\n",
    "                while next_flush in results_by_idx:\n",
    "                    out = results_by_idx.pop(next_flush)\n",
    "                    if isinstance(out, Exception):\n",
    "                        errors += 1\n",
    "                    else:\n",
    "                        rec2, did_patch = out\n",
    "                        if did_patch:\n",
    "                            patched += 1\n",
    "                            fpatch.write(json.dumps(rec2, ensure_ascii=False) + \"\\n\")\n",
    "                    next_flush += 1\n",
    "                \n",
    "                pbar.set_postfix(cands=trunc_candidates, patched=patched, errs=errors)\n",
    "\n",
    "    print(f\"Done processing {in_path.name}.\")\n",
    "    print(f\"Patched records written: {patched} | Errors: {errors}\")\n",
    "\n",
    "    if DO_MERGE:\n",
    "        patch_map = {record_key(r): r for r in iter_jsonl(patch_path)}\n",
    "        if merged_path.exists():\n",
    "            merged_path.unlink()\n",
    "\n",
    "        replaced = 0\n",
    "        with merged_path.open(\"a\", encoding=\"utf-8\") as fmerged:\n",
    "            for r in iter_jsonl(in_path):\n",
    "                k = record_key(r)\n",
    "                if k in patch_map:\n",
    "                    r = patch_map[k]\n",
    "                    replaced += 1\n",
    "                fmerged.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        print(f\"Merged output saved to: {merged_path}\")\n",
    "        print(f\"Total replaced records in merge: {replaced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57ae6f-db29-4804-b94e-92ba0bd18c1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
